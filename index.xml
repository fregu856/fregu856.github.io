<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Fredrik K. Gustafsson | PhD Student in Probabilistic Deep Learning on Fredrik K. Gustafsson | PhD Student in Probabilistic Deep Learning</title>
    <link>/</link>
    <description>Recent content in Fredrik K. Gustafsson | PhD Student in Probabilistic Deep Learning on Fredrik K. Gustafsson | PhD Student in Probabilistic Deep Learning</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright &amp;copy; Fredrik K. Gustafsson 2021</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0200</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Learning Proposals for Practical Energy-Based Regression</title>
      <link>/publication/ebms_proposals/</link>
      <pubDate>Fri, 22 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>/publication/ebms_proposals/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Uncertainty-Aware Body Composition Analysis with Deep Regression Ensembles on UK Biobank MRI</title>
      <link>/publication/mri_regression/</link>
      <pubDate>Mon, 18 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/publication/mri_regression/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Deep Energy-Based NARX Models</title>
      <link>/publication/ebms_narx/</link>
      <pubDate>Wed, 09 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/publication/ebms_narx/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Accurate 3D Object Detection using Energy-Based Models</title>
      <link>/publication/ebms_3dod/</link>
      <pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/publication/ebms_3dod/</guid>
      <description></description>
    </item>
    
    <item>
      <title>How to Train Your Energy-Based Model for Regression</title>
      <link>/publication/ebms_regression/</link>
      <pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/publication/ebms_regression/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Energy-Based Models for Deep Probabilistic Regression</title>
      <link>/publication/dctd/</link>
      <pubDate>Thu, 26 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/dctd/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Evaluating Scalable Bayesian Deep Learning Methods for Robust Computer Vision</title>
      <link>/publication/evaluating_bdl/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/evaluating_bdl/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Getting started with PyTorch:&lt;br&gt; 1 - Linear regression</title>
      <link>/post/19apr/</link>
      <pubDate>Thu, 25 Apr 2019 00:00:00 +0200</pubDate>
      
      <guid>/post/19apr/</guid>
      <description>

&lt;p&gt;&lt;em&gt;All code found in this blog post is also available on &lt;a href=&#34;https://colab.research.google.com/drive/1UfqfzEvaq18a2b8Y7kHTU8iAfOVzbF6t&#34; target=&#34;_blank&#34;&gt;Google Colab&lt;/a&gt; where it can be executed directly in the browser.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;When I first got interested in deep learning a couple of years ago, I started out using &lt;a href=&#34;https://www.tensorflow.org/&#34; target=&#34;_blank&#34;&gt;TensorFlow&lt;/a&gt;. In early 2018 I then decided to switch to &lt;a href=&#34;https://pytorch.org/&#34; target=&#34;_blank&#34;&gt;PyTorch&lt;/a&gt;, a decision that I&amp;rsquo;ve been very happy with ever since. Today, the difference between the two frameworks is probably quite small in practice (and both are extensively used by researchers in the field), but I &lt;em&gt;personally&lt;/em&gt; still find PyTorch more convenient to use.&lt;/p&gt;

&lt;p&gt;In a short series of blog posts I thus intend to try and help anyone interested in the field to get started with PyTorch and deep learning, by providing (hopefully) clearly written code examples.&lt;/p&gt;

&lt;p&gt;In this first post, we&amp;rsquo;ll start with perhaps the most simple example problem there is and try to fit a straight line to some noisy data points. We will however do so using &lt;a href=&#34;http://ruder.io/optimizing-gradient-descent/index.html#gradientdescentvariants&#34; target=&#34;_blank&#34;&gt;mini-batch Stochastic Gradient Descent (SGD)&lt;/a&gt; and use the same basic code structure that can be used also for significantly more interesting problems, such as &lt;a href=&#34;https://github.com/fregu856/deeplabv3&#34; target=&#34;_blank&#34;&gt;street-scene semantic segmentation&lt;/a&gt; or &lt;a href=&#34;https://github.com/fregu856/3DOD_thesis&#34; target=&#34;_blank&#34;&gt;automotive 3D object detection&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Essentially, we need to specify just three things: a dataset class, a model class and a training loop.&lt;/p&gt;

&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;

&lt;p&gt;For our dataset class, we need to specify a constructor and two member functions.&lt;/p&gt;

&lt;p&gt;In the constructor &lt;code&gt;__init__&lt;/code&gt;, we create a synthetic dataset $D = \{(x_1, y_1), \dots, (x_N, y_N)\}$ by drawing $x_i$ uniformly from an interval and adding Gaussian noise to a given straight line:
\[
x_i \sim U[-3, 3], \quad i = 1, \dots, N,
\]
\[
y_i = \bar{k}x_i + \bar{m} + \epsilon_i, \quad \epsilon_i \sim \mathcal{N}(0, 0.5), \quad i = 1, \dots, N.
\]&lt;/p&gt;

&lt;p&gt;We also assign the number of examples, $N$, to a member variable so that it can be returned by &lt;code&gt;__len__&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Finally, in &lt;code&gt;__getitem__&lt;/code&gt; we return the corresponding example $(x_i, y_i)$ given an index $i \in \{1, \dots, N\}$.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
import torch
import torch.utils.data

import numpy as np

import matplotlib.pyplot as plt

class LinearRegressionDataset(torch.utils.data.Dataset):
    def __init__(self, k, m, N):
        self.X = np.random.uniform(low=-3.0, high=3.0, size=(N, )) # (shape: (N, ))

        epsilon = np.random.normal(loc=0.0, scale=0.5, size=(N, )) # (shape: (N, ))

        self.Y = k*self.X + m + epsilon # (shape: (N, ))

        plt.figure(figsize=(8, 6))
        plt.plot(self.X, self.Y, &amp;quot;^k&amp;quot;, label=&amp;quot;Training data examples&amp;quot;)
        plt.plot([-3, 3], [k*(-3)+m, k*3+m], &amp;quot;r&amp;quot;, label=&amp;quot;True straight line&amp;quot;)
        plt.legend()
        plt.ylabel(&amp;quot;y&amp;quot;)
        plt.xlabel(&amp;quot;x&amp;quot;)
        plt.show()

        self.num_examples = N

        self.X = self.X.astype(np.float32)
        self.Y = self.Y.astype(np.float32)

    def __getitem__(self, index):
        x = self.X[index]

        y = self.Y[index]

        return (x, y)

    def __len__(self):
        return self.num_examples

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In &lt;code&gt;__init__&lt;/code&gt;, we also plot our created dataset $D = \{(x_1, y_1), \dots, (x_N, y_N)\}$ together with the true straight line $y = \bar{k}x + \bar{m}$. For $\bar{k} = 3$, $\bar{m}= 5$, $N= 50$, we get the following plot:
&lt;img src=&#34;/img/19apr/1.png&#34; alt=&#34;Plot of our training dataset&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;model&#34;&gt;Model&lt;/h3&gt;

&lt;p&gt;For our model class, we need to specify a constructor and one member function.&lt;/p&gt;

&lt;p&gt;In the constructor &lt;code&gt;__init__&lt;/code&gt;, we create our two model parameters $k, m$ and assign them to member variables. These are then used in &lt;code&gt;forward&lt;/code&gt; to output a prediction $\hat{y} = kx + m$ for a given input $x$.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
import torch.nn as nn

class LinearRegressionModel(nn.Module):
    def __init__(self):
        super(LinearRegressionModel, self).__init__()

        self.k = nn.Parameter(torch.zeros(1)) # (shape: (1))
        self.m = nn.Parameter(torch.zeros(1)) # (shape: (1))

    def forward(self, x):
        # (x has shape: (batch_size))

        y_hat = self.k*x + self.m # (shape: (batch_size))

        return y_hat

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;training-loop&#34;&gt;Training loop&lt;/h3&gt;

&lt;p&gt;To train our model on the dataset using SGD, we create instances of &lt;code&gt;LinearRegressionDataset&lt;/code&gt; and &lt;code&gt;LinearRegressionModel&lt;/code&gt;, we create the data loader &lt;code&gt;train_loader&lt;/code&gt; (which will repeatedly call &lt;code&gt;LinearRegressionDataset.__getitem__&lt;/code&gt; to create mini-batches), and create the SGD optimizer object &lt;code&gt;optimizer&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We then iterate through the entire dataset with mini-batches of size &lt;code&gt;batch_size&lt;/code&gt;, and for each mini-batch we compute the L2 loss $L(k, m) = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2,$ we compute the gradients of this mini-batch loss with respect to our model parameters, and finally use these gradients in the SGD step to update the parameters.&lt;/p&gt;

&lt;p&gt;One such iteration through the dataset is called an epoch, and we repeat this process &lt;code&gt;num_epochs&lt;/code&gt; times.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
from torch.autograd import Variable

num_epochs = 25
learning_rate = 0.01
batch_size = 8

k = 3
m = 5
N = 50

train_dataset = LinearRegressionDataset(k=k, m=m, N=N)

num_train_batches = int(len(train_dataset)/batch_size)
print (&amp;quot;num train batches per epoch: %d&amp;quot; % num_train_batches)

train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                           batch_size=batch_size, shuffle=True,
                                           num_workers=1)

model = LinearRegressionModel()
model = model.cuda()
model.train() # (set in training mode, this affects BatchNorm and dropout)

optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

epoch_losses_train = []
for epoch in range(num_epochs):
    print (&amp;quot;###########################&amp;quot;)
    print (&amp;quot;epoch: %d/%d&amp;quot; % (epoch+1, num_epochs))

    batch_losses = []
    for step, (x, y) in enumerate(train_loader):
        x = Variable(x).cuda() # (shape: (batch_size))
        y = Variable(y).cuda() # (shape: (batch_size))

        y_hat = model(x) # (shape: (batch_size))

        loss = torch.mean(torch.pow(y - y_hat, 2))

        # optimization step:
        optimizer.zero_grad() # (reset gradients)
        loss.backward() # (compute gradients)
        optimizer.step() # (perform the SGD update of the model parameters)

        # store the loss value:
        loss_value = loss.data.cpu().numpy()
        batch_losses.append(loss_value)

    epoch_loss = np.mean(batch_losses)
    epoch_losses_train.append(epoch_loss)
    print (&amp;quot;train loss: %g&amp;quot; % epoch_loss)

plt.figure(figsize=(8, 6))
plt.plot(epoch_losses_train, &amp;quot;^k&amp;quot;)
plt.plot(epoch_losses_train, &amp;quot;k&amp;quot;)
plt.ylabel(&amp;quot;Loss&amp;quot;)
plt.xlabel(&amp;quot;Epoch&amp;quot;)
plt.title(&amp;quot;Loss per epoch&amp;quot;)
plt.show()

print (&amp;quot;k, true value: %g, estimated value: %g&amp;quot; % (k, model.k))
print (&amp;quot;m, true value: %g, estimated value: %g&amp;quot; % (m, model.m))

plt.figure(figsize=(8, 6))
plt.plot(train_dataset.X, train_dataset.Y, &amp;quot;^k&amp;quot;, label=&amp;quot;Training data examples&amp;quot;)
plt.plot([-3, 3], [k*(-3)+m, k*3+m], &amp;quot;r&amp;quot;, label=&amp;quot;True straight line&amp;quot;)
plt.plot([-3, 3], [model.k*(-3)+model.m, model.k*3+model.m], &amp;quot;b&amp;quot;,
         label=&amp;quot;Estimated straight line&amp;quot;)
plt.legend()
plt.ylabel(&amp;quot;y&amp;quot;)
plt.xlabel(&amp;quot;x&amp;quot;)
plt.show()

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In each epoch, we also store all mini-batch losses and then average them to get a loss value for the entire dataset. Once training is completed, we plot these epoch losses:
&lt;img src=&#34;/img/19apr/2.png&#34; alt=&#34;Plot of the training loss per epoch&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As we can see, the loss quickly decreases in the beginning and then starts to level out as our model parameters $k, m$ get close to the true values $\bar{k} = 3$, $\bar{m}= 5$. We also plot our estimated straight line $\hat{y} = kx + m$ and compare it to the true one:
&lt;img src=&#34;/img/19apr/3.png&#34; alt=&#34;Plot of our training dataset, the true straight line and our estimated straight line&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;additional-visualizations&#34;&gt;Additional visualizations&lt;/h3&gt;

&lt;p&gt;Since we in this simple example problem have just two model parameters, we can gain additional insight by computing our loss function $L(k, m) = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2$ (over the entire dataset) for different values of $k, m$ and plot the loss surface:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
num_values=100
k_plot_values = np.linspace(start=(k-7.0), stop=(k+7.0), num=num_values)
m_plot_values = np.linspace(start=(m-7.0), stop=(m+7.0), num=num_values)
# (k_plot_values and m_plot_values both have shape: (num_values, ))
loss_values = np.zeros((num_values, num_values))
for k_i in range(num_values):
    for m_i in range(num_values):
        Y_hat = k_plot_values[k_i]*train_dataset.X + m_plot_values[m_i]
        # (Y_hat has shape: (N, ))

        loss = np.mean((train_dataset.Y - Y_hat)**2)
        loss_values[m_i, k_i] = loss

plt.figure(figsize=(8, 6))
K, M = np.meshgrid(k_plot_values, m_plot_values)
plt.contour(K, M, loss_values, levels=20)
plt.plot(k, m, &amp;quot;r*&amp;quot;, label=&amp;quot;True parameters&amp;quot;)
plt.legend()
plt.ylabel(&amp;quot;m&amp;quot;)
plt.xlabel(&amp;quot;k&amp;quot;)
plt.title(&amp;quot;Loss surface&amp;quot;)
plt.show()

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/img/19apr/4.png&#34; alt=&#34;Plot of the loss surface&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Finally, we can also store the current values of $k, m$ at different points during training and plot the parameter trajectory, starting at the initial point $(k_0 = 0, m_0 = 0)$ and ending at our final estimate $(k = 3.03, m = 4.86)$:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
model = LinearRegressionModel()
model = model.cuda()
model.train() # (set in training mode, this affects BatchNorm and dropout)

optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

epoch_losses_train = []
k_values = []
m_values = []
k_values.append(model.k.data.cpu().numpy())
m_values.append(model.m.data.cpu().numpy())
for epoch in range(num_epochs):
    print (&amp;quot;###########################&amp;quot;)
    print (&amp;quot;epoch: %d/%d&amp;quot; % (epoch+1, num_epochs))

    batch_losses = []
    for step, (x, y) in enumerate(train_loader):
        x = Variable(x).cuda() # (shape: (batch_size))
        y = Variable(y).cuda() # (shape: (batch_size))

        y_hat = model(x) # (shape: (batch_size))

        loss = torch.mean(torch.pow(y - y_hat, 2))

        # optimization step:
        optimizer.zero_grad() # (reset gradients)
        loss.backward() # (compute gradients)
        optimizer.step() # (perform the SGD update of the model parameters)

        # store the loss value:
        loss_value = loss.data.cpu().numpy()
        batch_losses.append(loss_value)

    epoch_loss = np.mean(batch_losses)
    epoch_losses_train.append(epoch_loss)
    print (&amp;quot;train loss: %g&amp;quot; % epoch_loss)

    k_values.append(model.k.data.cpu().numpy())
    m_values.append(model.m.data.cpu().numpy())

plt.figure(figsize=(8, 6))
plt.plot(epoch_losses_train, &amp;quot;^k&amp;quot;)
plt.plot(epoch_losses_train, &amp;quot;k&amp;quot;)
plt.ylabel(&amp;quot;Loss&amp;quot;)
plt.xlabel(&amp;quot;Epoch&amp;quot;)
plt.title(&amp;quot;Loss per epoch&amp;quot;)
plt.show()

print (&amp;quot;k, true value: %g, estimated value: %g&amp;quot; % (k, model.k))
print (&amp;quot;m, true value: %g, estimated value: %g&amp;quot; % (m, model.m))

plt.figure(figsize=(8, 6))
plt.plot(train_dataset.X, train_dataset.Y, &amp;quot;^k&amp;quot;, label=&amp;quot;Training data examples&amp;quot;)
plt.plot([-3, 3], [k*(-3)+m, k*3+m], &amp;quot;r&amp;quot;, label=&amp;quot;True straight line&amp;quot;)
plt.plot([-3, 3], [model.k*(-3)+model.m, model.k*3+model.m], &amp;quot;b&amp;quot;,
         label=&amp;quot;Estimated straight line&amp;quot;)
plt.legend()
plt.ylabel(&amp;quot;y&amp;quot;)
plt.xlabel(&amp;quot;x&amp;quot;)
plt.show()

num_values=100
k_plot_values = np.linspace(start=(k-7.0), stop=(k+7.0), num=num_values)
m_plot_values = np.linspace(start=(m-7.0), stop=(m+7.0), num=num_values)
# (k_plot_values and m_plot_values both have shape: (num_values, ))
loss_values = np.zeros((num_values, num_values))
for k_i in range(num_values):
    for m_i in range(num_values):
        Y_hat = k_plot_values[k_i]*train_dataset.X + m_plot_values[m_i]
        # (Y_hat has shape: (N, ))

        loss = np.mean((train_dataset.Y - Y_hat)**2)
        loss_values[m_i, k_i] = loss

plt.figure(figsize=(8, 6))
K, M = np.meshgrid(k_plot_values, m_plot_values)
plt.contour(K, M, loss_values, levels=20)
plt.plot(k, m, &amp;quot;r*&amp;quot;, label=&amp;quot;True parameters&amp;quot;)
plt.plot(k_values, m_values, &amp;quot;xb&amp;quot;, label=&amp;quot;Estimated parameters&amp;quot;)
plt.plot(k_values, m_values, &amp;quot;b&amp;quot;)
plt.legend()
plt.ylabel(&amp;quot;m&amp;quot;)
plt.xlabel(&amp;quot;k&amp;quot;)
plt.title(&amp;quot;Parameter trajectory&amp;quot;)
plt.show()

plt.figure(figsize=(8, 6))
K, M = np.meshgrid(k_plot_values, m_plot_values)
plt.contour(K, M, loss_values, levels=20)
plt.plot(k, m, &amp;quot;r*&amp;quot;, label=&amp;quot;True parameters&amp;quot;)
plt.plot(k_values, m_values, &amp;quot;xb&amp;quot;, label=&amp;quot;Estimated parameters&amp;quot;)
plt.plot(k_values, m_values, &amp;quot;b&amp;quot;)
plt.legend()
plt.ylabel(&amp;quot;m&amp;quot;)
plt.xlabel(&amp;quot;k&amp;quot;)
plt.xlim([k-3.0, k+3.0])
plt.ylim([m-3.0, m+3.0])
plt.title(&amp;quot;Parameter trajectory - Zoomed&amp;quot;)
plt.show()

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/img/19apr/7.png&#34; alt=&#34;Plot of the trajectory of the model parameters during training&#34; /&gt;
&lt;img src=&#34;/img/19apr/8.png&#34; alt=&#34;Zoomed plot of the trajectory of the model parameters during training&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Feel free to &lt;a href=&#34;/#contact&#34;&gt;contact&lt;/a&gt; me or post a comment below if you have any questions or comments. In the next blog post, we&amp;rsquo;ll do nonlinear curve-fitting using feed-forward neural networks.&lt;/em&gt;&lt;/p&gt;

&lt;!-- ```console
num train batches per epoch: 6
###########################
epoch: 1/25
train loss: 50.0857
###########################
epoch: 2/25
train loss: 25.0836
``` --&gt;
</description>
    </item>
    
    <item>
      <title>PyTorch Implementation of DeepLabV3</title>
      <link>/project/deeplabv3/</link>
      <pubDate>Sat, 01 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/project/deeplabv3/</guid>
      <description>&lt;p&gt;PyTorch implementation of &lt;a href=&#34;https://arxiv.org/abs/1706.05587&#34; target=&#34;_blank&#34;&gt;DeepLabV3&lt;/a&gt;, trained on the Cityscapes dataset. Please see the GitHub repository linked below for code and further details.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Stinspira</title>
      <link>/project/stinspira/</link>
      <pubDate>Sun, 01 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/project/stinspira/</guid>
      <description>&lt;p&gt;Website (in Swedish) aiming to increase interest in higher education among youths. Web design, logo design and content creation. &lt;a href=&#34;http://www.stinspira.se/&#34; target=&#34;_blank&#34;&gt;stinspira.se&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Automotive 3D Object Detection Without Target Domain Annotations</title>
      <link>/publication/msc/</link>
      <pubDate>Fri, 01 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/msc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Skalman</title>
      <link>/project/skalman/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/project/skalman/</guid>
      <description>&lt;p&gt;Ongoing project.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SMAUGS</title>
      <link>/project/smaugs/</link>
      <pubDate>Fri, 01 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/project/smaugs/</guid>
      <description>&lt;p&gt;By Fredrik K. Gustafsson, Andreas Hägglund, Andreas Lundgren, Elin Näsholm, Fredrik Tormod, Hampus Andersson, Jonathan Jerner and Mattias Andreasson.&lt;/p&gt;
&lt;p&gt;My main responsibilities: SLAM and ROS.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2D Object Detection for Autonomous Driving</title>
      <link>/project/2dod/</link>
      <pubDate>Sat, 02 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/project/2dod/</guid>
      <description>&lt;p&gt;TensorFlow implementation of SqueezeDet, trained on the KITTI dataset. The results in the video above can obviously be improved, but because of limited computing resources (personally funded Azure VM) I did not perform any further hyperparameter tuning. Please see the GitHub repository linked below for code and further details.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Semantic Segmentation for Autonomous Driving</title>
      <link>/project/segmentation/</link>
      <pubDate>Fri, 01 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/project/segmentation/</guid>
      <description>&lt;p&gt;TensorFlow implementation of ENet, trained on the Cityscapes dataset. The results in the video above can obviously be improved, but because of limited computing resources (personally funded Azure VM) I did not perform any further hyperparameter tuning. Please see the GitHub repository linked below for code and further details.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
